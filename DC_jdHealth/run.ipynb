{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad4dd48-3a4a-4b62-9701-595b14983ae4",
   "metadata": {},
   "source": [
    "# 数据集构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9826430-851e-446f-adda-9db40764a2ea",
   "metadata": {},
   "source": [
    "## data_provider/uea.py\n",
    "- 数据集构造时一些功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990b2366-6811-4afb-8092-552a39a2be5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data, max_len=None):\n",
    "    '''\n",
    "    用于规范各bs的维度（用0填补）,并返回padding_masks（告知那些位置是填补的）\n",
    "    '''\n",
    "    batch_size = len(data)\n",
    "    features, labels = zip(*data)\n",
    "    \n",
    "    # 添加批量为维度\n",
    "    lengths = [X.shape[0] for X in features]  # lengths = seq_len\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "        \n",
    "    # (padded_length, feat_dim) -> (batch_size, padded_length, feat_dim)\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1]) \n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "        \n",
    "    # (batch_size, num_labels)\n",
    "    targets = torch.stack(labels, dim=0)\n",
    "    \n",
    "    # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16),max_len=max_len)\n",
    "    return X, targets, padding_masks\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    用一个布尔矩阵，指明填充的位置（填充的是seq_len，不是特征维度)\n",
    "    所以返回的是 [bs, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()  # 获取批量大小\n",
    "    max_len = max_len or lengths.max_val()\n",
    "    return (torch.arange(0, max_len, device=lengths.device).type_as(lengths).repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"\n",
    "    Normalizes dataframe across ALL contained rows (time steps). Different from per-sample normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, norm_type='standardization', mean=None, std=None, min_val=None, max_val=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            norm_type: choose from:\n",
    "                \"standardization\", \"minmax\": normalizes dataframe across ALL contained rows (time steps)\n",
    "                \"per_sample_std\", \"per_sample_minmax\": normalizes each sample separately (i.e. across only its own rows)\n",
    "            mean, std, min_val, max_val: optional (num_feat,) Series of pre-computed values\n",
    "        \"\"\"\n",
    "        self.norm_type = norm_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: input dataframe\n",
    "        Returns:\n",
    "            df: normalized dataframe\n",
    "        \"\"\"\n",
    "        if self.norm_type == \"standardization\":\n",
    "            if self.mean is None:\n",
    "                self.mean = df.mean()\n",
    "                self.std = df.std()\n",
    "            return (df - self.mean) / (self.std + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"minmax\":\n",
    "            if self.max_val is None:\n",
    "                self.max_val = df.max()\n",
    "                self.min_val = df.min()\n",
    "            return (df - self.min_val) / (self.max_val - self.min_val + np.finfo(float).eps)\n",
    "\n",
    "        elif self.norm_type == \"per_sample_std\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            return (df - grouped.transform('mean')) / grouped.transform('std')\n",
    "\n",
    "        elif self.norm_type == \"per_sample_minmax\":\n",
    "            grouped = df.groupby(by=df.index)\n",
    "            min_vals = grouped.transform('min')\n",
    "            return (df - min_vals) / (grouped.transform('max') - min_vals + np.finfo(float).eps)\n",
    "\n",
    "        else:\n",
    "            raise (NameError(f'Normalize method \"{self.norm_type}\" not implemented'))\n",
    "\n",
    "def interpolate_missing(y):\n",
    "    \"\"\"\n",
    "    Replaces NaN values in pd.Series `y` using linear interpolation\n",
    "    \"\"\"\n",
    "    if y.isna().any():\n",
    "        y = y.interpolate(method='linear', limit_direction='both')\n",
    "    return y\n",
    "\n",
    "\n",
    "def subsample(y, limit=256, factor=2):\n",
    "    \"\"\"\n",
    "    If a given Series is longer than `limit`, returns subsampled sequence by the specified integer factor\n",
    "    \"\"\"\n",
    "    if len(y) > limit:\n",
    "        return y[::factor].reset_index(drop=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bcef5-51f1-4b0b-9cc3-4e9f0cceb32f",
   "metadata": {},
   "source": [
    "## data_provider/data_loader.py\n",
    "- 数据集制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36584a57-8b19-42b8-acef-1b0d02a2bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.timefeatures import time_features\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from data_provider.uea import subsample, interpolate_missing, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12f33ca-a745-40d9-af27-d00b55818917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset_classifier(Dataset):\n",
    "    def __init__(self, args, flag=None):\n",
    "        self.args = args    \n",
    "        self.flag = flag\n",
    "        self.root_path = self.args.root_path\n",
    "        self.all_df, self.labels_df = self.load_data()\n",
    "        normalizer = Normalizer()\n",
    "        self.feature_df = normalizer.normalize(self.all_df)\n",
    "        \n",
    "    def load_data(self):\n",
    "        if self.flag == \"train\":\n",
    "            self.data_path = \"train_data.npy\"\n",
    "            self.label_path = \"train_label.csv\"\n",
    "            df = np.load(os.path.join(self.root_path, self.data_path))\n",
    "            label = pd.read_csv(os.path.join(self.root_path, self.label_path)).values\n",
    "            \n",
    "        elif self.flag == \"val\":\n",
    "            self.data_path = \"val_data.npy\"\n",
    "            self.label_path = \"val_label.csv\"\n",
    "            df = np.load(os.path.join(self.root_path, self.data_path))\n",
    "            label = pd.read_csv(os.path.join(self.root_path, self.label_path)).values\n",
    "            \n",
    "        elif self.flag == \"test\":\n",
    "            self.data_path = \"test_data.npy\"\n",
    "            df = np.load(os.path.join(self.root_path, self.data_path))\n",
    "            label = None\n",
    "        else:\n",
    "            print(\"请指定读取的文件\")\n",
    "            \n",
    "        if self.flag in [\"train\", \"val\"]:\n",
    "            print(label.shape)\n",
    "        return df, label\n",
    "\n",
    "    def instance_norm(self, case):\n",
    "        mean = case.mean(0, keepdim=True)\n",
    "        case = case - mean\n",
    "        stdev = torch.sqrt(torch.var(case, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        case /= stdev\n",
    "        return case\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        data = self.feature_df[ind]   # [seq_len,n_features]\n",
    "        \n",
    "        if self.labels_df is not None:\n",
    "            label = self.labels_df[ind]\n",
    "        else:\n",
    "            label = np.zeros(1) \n",
    "        return self.instance_norm(torch.from_numpy(data)), torch.from_numpy(label).reshape(-1,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976bfa07-a661-449c-9df4-333acd03de96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11106, 1)\n",
      "11106 \n",
      " 2 \n",
      " torch.Size([180, 42]) \n",
      " torch.Size([1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.root_path = \"./user_data/\"\n",
    "        self.batch_size = 128\n",
    "        \n",
    "args = Config()\n",
    "data = MyDataset_classifier(args = args, flag=\"train\")\n",
    "\n",
    "print(len(data), \"\\n\",   # 原文件大小是2877305，因为sep_len是5，所以少了5\n",
    "      len(data[0]), \"\\n\", \n",
    "      data[0][0].shape,\"\\n\", \n",
    "      data[0][1].shape, \"\\n\", \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ad31b-c8bd-40af-8970-046f5db9a830",
   "metadata": {},
   "source": [
    "# data_provider/data_factory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584a606e-89ca-4158-9e7a-b84da352b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a660b45-b1a4-4346-a849-19c85d864c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_provider(args, flag):\n",
    "    shuffle_flag = False if flag == 'test' else True\n",
    "    drop_last = False if flag == 'test' else True\n",
    "    \n",
    "    data_set = MyDataset_classifier(\n",
    "        args = args,\n",
    "        flag = flag\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=args.batch_size,  # bs还是放到参数里面指定\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=0,              # 直接写死吧\n",
    "        drop_last=drop_last,        # 最后的数据组不成一个bs，就舍弃\n",
    "        collate_fn=lambda x: collate_fn(x)\n",
    "    )\n",
    "    return data_set, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4105bacf-80ff-467a-9bb1-a288286c111d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11106, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 180, 42]), torch.Size([128, 1]), torch.Size([128, 180]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1, r2 = data_provider(args, \"train\")\n",
    "\n",
    "for e1, i in enumerate(r2):\n",
    "    if e1==1:\n",
    "        break\n",
    "        \n",
    "i[0].shape, i[1].shape, i[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff71f5-634d-4e71-94ac-9155fef876a0",
   "metadata": {},
   "source": [
    "# my_model/TimesNet.py\n",
    "- 对预测类任务\n",
    "    - 因为模型好像都是seq-to-seq模型，所以c_out = enc_in = len(features)（其实好像不用，但是c_out要大于enc_in)，因为模型输出最后设置了只要pred_len那一段\n",
    "- 对分类任务\n",
    "    - enc_in = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c2f711-7a06-4238-a4d9-5dde26c7acc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c9802c-5594-4675-a491-f2a9cf3e3389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FFT_for_Period(x, k=2):\n",
    "    '''\n",
    "    提取时间序列数据中的主要周期成分和对应周期的权重\n",
    "    '''\n",
    "    # [B, T, C]\n",
    "    xf = torch.fft.rfft(x, dim=1)  # 得到频域\n",
    "    # find period by amplitudes\n",
    "    frequency_list = abs(xf).mean(0).mean(-1)  # 对第一个和最后一个维度求均值，只剩下seq_len\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = x.shape[1] // top_list\n",
    "    return period, abs(xf).mean(-1)[:, top_list]\n",
    "\n",
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.k = configs.top_k\n",
    "        \n",
    "        # parameter-efficient design\n",
    "        self.conv = nn.Sequential(\n",
    "            Inception_Block_V1(configs.d_model, configs.d_ff,num_kernels=configs.num_kernels),\n",
    "            nn.GELU(),\n",
    "            Inception_Block_V1(configs.d_ff, configs.d_model,num_kernels=configs.num_kernels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, N = x.size()\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "        \n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            \n",
    "            # padding ：将输入时间序列的长度 T 补齐到一个 period 的整数倍\n",
    "            if T % period != 0:\n",
    "                length = ((T // period) + 1) * period\n",
    "                padding = torch.zeros([x.shape[0], (length - T), x.shape[2]]).to(x.device)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "            else:\n",
    "                length = T\n",
    "                out = x\n",
    "                \n",
    "            # reshape\n",
    "            out = out.reshape(B, length // period, period, N).permute(0, 3, 1, 2).contiguous()\n",
    "            \n",
    "            # 2D conv: from 1d Variation to 2d Variation\n",
    "            out = self.conv(out)\n",
    "            \n",
    "            # 重塑输出，特征维度不变\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "            res.append(out[:, :length, :]) # 为啥没有label_len\n",
    "            \n",
    "        res = torch.stack(res, dim=-1)  # 维度：（B,T,N,self.k）\n",
    "        \n",
    "        # adaptive aggregation\n",
    "        period_weight = F.softmax(period_weight, dim=1)\n",
    "        period_weight = period_weight.unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "        \n",
    "        res = torch.sum(res * period_weight, -1)  # 乘上对应频段的权重\n",
    "        res = res + x                             # residual connection\n",
    "        return res\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper link: https://openreview.net/pdf?id=ju_Uqw384Oq\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.task_name = configs.task_name\n",
    "        \n",
    "        self.seq_len = configs.seq_len\n",
    "        \n",
    "        # 分类任务用不上pred_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        \n",
    "        # 这里其实没用上label_len，因为这个模型不decoder\n",
    "        self.label_len = configs.label_len\n",
    "        \n",
    "        # 这里的enc_in = len(features)/特征个数\n",
    "        self.model = nn.ModuleList([TimesBlock(configs)for _ in range(configs.e_layers)])\n",
    "        \n",
    "        # 因为我不使用x_mark_enc，所以configs.embed，configs.freq都用不上\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed,\n",
    "                                           configs.freq, configs.dropout)\n",
    "        self.layer = configs.e_layers\n",
    "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "        \n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.predict_linear = nn.Linear(self.seq_len, self.pred_len + self.seq_len)\n",
    "            self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "            \n",
    "        if self.task_name == 'imputation' or self.task_name == 'anomaly_detection':\n",
    "            self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "            \n",
    "        if self.task_name == 'classification':\n",
    "            self.act = F.gelu\n",
    "            self.dropout = nn.Dropout(configs.dropout)\n",
    "            self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n",
    "            \n",
    "    def classification(self, x_enc, x_mark_enc):\n",
    "        '''\n",
    "        这里的x_mark_enc，是填充标志矩阵。和预测任务中的x_mark_enc（时间特征）不一样\n",
    "        '''\n",
    "        \n",
    "        # 这里也是把x_mark_enc定为None，那么freq和embed都没必要传入\n",
    "        enc_out = self.enc_embedding(x_enc, None)  # [B,T,C]\n",
    "        \n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "        \n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # zero-out padding embeddings\n",
    "        output = output * x_mark_enc.unsqueeze(-1)\n",
    "        \n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        if self.task_name == 'imputation':\n",
    "            dec_out = self.imputation(\n",
    "                x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\n",
    "            return dec_out  # [B, L, D]\n",
    "        if self.task_name == 'anomaly_detection':\n",
    "            dec_out = self.anomaly_detection(x_enc)\n",
    "            return dec_out  # [B, L, D]\n",
    "        if self.task_name == 'classification':\n",
    "            dec_out = self.classification(x_enc, x_mark_enc)\n",
    "            return dec_out  # [B, N]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7f609-fda0-452f-8af1-ccaa010771e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# exp/exp_basic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4824618f-6242-45e6-b5fb-1ad9c3c34f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# from models import Autoformer, Transformer, TimesNet, Nonstationary_Transformer, DLinear, FEDformer, \\\n",
    "#     Informer, LightTS, Reformer, ETSformer, Pyraformer, PatchTST, MICN, Crossformer, FiLM, iTransformer, \\\n",
    "#     Koopa, TiDE, FreTS, TimeMixer, TSMixer, SegRNN, MambaSimple, Mamba, TemporalFusionTransformer\n",
    "# from my_model import TimesNet\n",
    "\n",
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "\n",
    "    def _acquire_device(self):\n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
    "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
    "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a858a-575f-4368-9813-4583278ac515",
   "metadata": {},
   "source": [
    "# ./exp/classification.py\n",
    "- enc_in = features_dim\n",
    "- pred_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a592c3-463b-434d-996d-1b63217aa523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from data_provider.data_factory import data_provider\n",
    "# from exp.exp_basic import Exp_Basic\n",
    "\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate, cal_accuracy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pdb\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64556b1e-878e-4bd2-9094-032c83194c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Exp_Classification(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Classification, self).__init__(args)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Model(self.args).float()\n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        return model\n",
    "    \n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        # model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        model_optim = optim.RAdam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        '''\n",
    "        这个函数在train中被调用\n",
    "        '''\n",
    "        total_loss = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                \n",
    "                outputs = self.model(batch_x, padding_mask)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                loss = criterion(pred, label.long().squeeze().cpu())\n",
    "                total_loss.append(loss)\n",
    "\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        total_loss = np.average(total_loss)\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "        trues = trues.flatten().cpu().numpy()\n",
    "        accuracy = cal_accuracy(predictions, trues)\n",
    "        self.model.train()\n",
    "        return total_loss, accuracy\n",
    "    \n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='train')\n",
    "        vali_data, vali_loader = self._get_data(flag='val')\n",
    "        \n",
    "        path = os.path.join(self.args.checkpoints, setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "        \n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                \n",
    "                # 模型不该修改，不然这里也要改\n",
    "                # outputs = self.model(batch_x, padding_mask, None, None)\n",
    "                outputs = self.model(batch_x, padding_mask)\n",
    "                loss = criterion(outputs, label.long().squeeze(-1))\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 1 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "                model_optim.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss, val_accuracy = self.vali(vali_data, vali_loader, criterion)\n",
    "\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.3f} Vali Loss: {3:.3f} Vali Acc: {4:.3f}\"\n",
    "                .format(epoch + 1, train_steps, train_loss, vali_loss, val_accuracy))\n",
    "            \n",
    "            # 模型是在早停里面保存的\n",
    "            early_stopping(-val_accuracy, self.model, path, epoch)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "                \n",
    "            if (epoch + 1) % 3 == 0:  # 每3个epoch调整一次学习率\n",
    "                adjust_learning_rate(model_optim, epoch + 1, self.args)\n",
    "                \n",
    "        # 保存最后一次的模型（无论是否早停）\n",
    "        final_model_path = os.path.join(path, 'final_checkpoint.pth')\n",
    "        torch.save(self.model.state_dict(), final_model_path)\n",
    "        \n",
    "    def test(self, setting, test=1):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        if test:\n",
    "            print('loading model, 需要自定义checkpint文件名')\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint_10.pth')))\n",
    "        preds = []\n",
    "        trues = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, label, padding_mask) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                padding_mask = padding_mask.float().to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                outputs = self.model(batch_x, padding_mask)\n",
    "                preds.append(outputs.detach())\n",
    "                trues.append(label)\n",
    "\n",
    "        preds = torch.cat(preds, 0)\n",
    "        trues = torch.cat(trues, 0)\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(preds)  # (total_samples, num_classes) est. prob. for each class and sample\n",
    "        predictions = torch.argmax(probs, dim=1).cpu().numpy()  # (total_samples,) int class index for each sample\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        predictions_label = pd.Series(predictions).to_csv(folder_path+\"/\"+\"predictions_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a0297-46be-4d0b-8ce0-6f42e2293ef5",
   "metadata": {},
   "source": [
    "# run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dcf6d13-c9c3-4a88-846d-9aa680cb44fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "# from exp.exp_imputation import Exp_Imputation\n",
    "# from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "# from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "# from exp.exp_classification import Exp_Classification\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from utils.print_args import print_args\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6c4c16-cfba-477f-b163-4e5ecec2c151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          classification      Is Training:        0                   \n",
      "  Model:              TimesNet            \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Root Path:          ./user_data/        \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mclassification Task\u001b[0m\n",
      "  Seq Len:            180                 \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             42                  Dec In:             7                   \n",
      "  C Out:              2                   d model:            512                 \n",
      "  n heads:            5                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               512                 \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "  Output Attention:   0                   \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         256                 \n",
      "  Patience:           3                   Learning Rate:      0.0001              \n",
      "  Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            1                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>testing : classification_TimesNet_sl180_ll128_pl64_dm512_nh5_el2<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "loading model\n",
      "test shape: torch.Size([1155, 3]) torch.Size([1155, 1])\n",
      "accuracy:0.43376623376623374\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "参数解析：\n",
    "    - 做测试，就把is_training设置为0\n",
    "    - features，target ：都是forecasting任务特有的，但也不算，反正不好用\n",
    "    - freq，embed：如果输入的数据有时间戳，可以指定这两个参数做时间的embed，但是如果没传入时间戳特征，\n",
    "                    这两个参数不管就行（指定了也不用）\n",
    "'''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fix_seed = 2021\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='TimesNet')\n",
    "    \n",
    "    # basic config\n",
    "    parser.add_argument('--task_name', type=str, default='classification',\n",
    "                        help='task name, options:[long_term_forecast, short_term_forecast,imputation, classification, anomaly_detection]')\n",
    "    parser.add_argument('--is_training', type=int, default=0, help='status')\n",
    "    parser.add_argument('--model', type=str, default='TimesNet',\n",
    "                        help='model name, options: [Autoformer, Transformer, TimesNet]')\n",
    "    \n",
    "    # data loader\n",
    "    parser.add_argument('--root_path', type=str, default='./user_data/')\n",
    "    parser.add_argument('--features', type=str, default='MS',\n",
    "                        help='forecasting任务特有, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "    parser.add_argument('--target', type=str, default='OT', \n",
    "                        help='target feature in S or MS task')\n",
    "    parser.add_argument('--freq', type=str, default='h',\n",
    "                        help='不传入x_mark，freq用不上。freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "    \n",
    "    # forecasting task\n",
    "    # parser.add_argument('--seq_len', type=int, default=180, help='input sequence length')\n",
    "    parser.add_argument('--label_len', type=int, default=128, help='start token length')\n",
    "    parser.add_argument('--pred_len', type=int, default=64, help='prediction sequence length')\n",
    "    parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "    \n",
    "    # inputation task\n",
    "    parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "    \n",
    "    # anomaly detection task\n",
    "    parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "    \n",
    "    # classification task\n",
    "    parser.add_argument('--seq_len', type=int, default=180,\n",
    "                        help='说是forecasting的参数，但是classification也要用')\n",
    "    parser.add_argument('--num_class', type=int, default=3, help='几分类任务')\n",
    "    \n",
    "    # model define\n",
    "    parser.add_argument('--expand', type=int, default=2, help='expansion factor for Mamba')\n",
    "    parser.add_argument('--d_conv', type=int, default=4, help='conv kernel size for Mamba')\n",
    "    parser.add_argument('--top_k', type=int, default=5,\n",
    "                        help='for TimesBlock，提取前几个主要成分周期')\n",
    "    parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "    parser.add_argument('--enc_in', type=int, default=42,\n",
    "                        help='等于特征数，encoder input size')\n",
    "    parser.add_argument('--dec_in', type=int, default=7, \n",
    "                        help='TimesNet不使用decoder, decoder input size')\n",
    "    parser.add_argument('--c_out', type=int, default=2,\n",
    "                        help='inputation和forecasting任务的，输出维度设置为和enc_in一样，output size')\n",
    "    \n",
    "    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "    parser.add_argument('--n_heads', type=int, default=5,\n",
    "                        help='多头自注意机制的数量')\n",
    "    parser.add_argument('--e_layers', type=int, default=2,\n",
    "                        help='编码器由几层堆叠组成，每一层包括自注意力机制和前馈神经网络')\n",
    "    parser.add_argument('--d_layers', type=int, default=1,\n",
    "                        help='解码器由几层堆叠组成，每一层包括自注意力机制、编码器-解码器注意力机制和前馈神经网络')\n",
    "    parser.add_argument('--d_ff', type=int, default=512,\n",
    "                        help='前馈神经网络（Feed Forward Network）的隐藏层维度')\n",
    "    \n",
    "    parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "    parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "    parser.add_argument('--distil', action='store_false', default=True,\n",
    "                        help='是否使用蒸馏:将深层网络简化为较浅层网络的方法，以减少计算量和模型复杂度')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "    parser.add_argument('--embed', type=str, default='timeF',\n",
    "                        help='time features encoding, options:[timeF, fixed, learned]')\n",
    "    parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "    parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "    \n",
    "    parser.add_argument('--channel_independence', type=int, default=1,\n",
    "                        help='0: channel dependence 1: channel independence for FreTS model')\n",
    "    parser.add_argument('--decomp_method', type=str, default='moving_avg',\n",
    "                        help='method of series decompsition, only support moving_avg or dft_decomp')\n",
    "    parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')\n",
    "    parser.add_argument('--down_sampling_layers', type=int, default=0, help='num of down sampling layers')\n",
    "    parser.add_argument('--down_sampling_window', type=int, default=1, help='down sampling window size')\n",
    "    \n",
    "    parser.add_argument('--down_sampling_method', type=str, default=None,\n",
    "                        help='down sampling method, only support avg, max, conv')\n",
    "    parser.add_argument('--seg_len', type=int, default=48,\n",
    "                        help='the length of segmen-wise iteration of SegRNN')\n",
    "    \n",
    "    # optimization\n",
    "    parser.add_argument('--itr', type=int, default=1,help='实验次数')\n",
    "    parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=256, help='batch size of train input data')\n",
    "    parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "    parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "    parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "    parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "    \n",
    "    # GPU\n",
    "    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "    parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "    # de-stationary projector params\n",
    "    parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n",
    "                        help='hidden layer dimensions of projector (List)')\n",
    "    parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n",
    "    \n",
    "    # metrics (dtw)\n",
    "    parser.add_argument('--use_dtw', type=bool, default=False, \n",
    "                        help='the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)')\n",
    "    \n",
    "    # Augmentation\n",
    "    parser.add_argument('--augmentation_ratio', type=int, default=0, help=\"How many times to augment\")\n",
    "    parser.add_argument('--seed', type=int, default=2, help=\"Randomization seed\")\n",
    "    parser.add_argument('--jitter', default=False, action=\"store_true\", help=\"Jitter preset augmentation\")\n",
    "    parser.add_argument('--scaling', default=False, action=\"store_true\", help=\"Scaling preset augmentation\")\n",
    "    parser.add_argument('--permutation', default=False, action=\"store_true\", help=\"Equal Length Permutation preset augmentation\")\n",
    "    parser.add_argument('--randompermutation', default=False, action=\"store_true\", help=\"Random Length Permutation preset augmentation\")\n",
    "    parser.add_argument('--magwarp', default=False, action=\"store_true\", help=\"Magnitude warp preset augmentation\")\n",
    "    parser.add_argument('--timewarp', default=False, action=\"store_true\", help=\"Time warp preset augmentation\")\n",
    "    parser.add_argument('--windowslice', default=False, action=\"store_true\", help=\"Window slice preset augmentation\")\n",
    "    parser.add_argument('--windowwarp', default=False, action=\"store_true\", help=\"Window warp preset augmentation\")\n",
    "    parser.add_argument('--rotation', default=False, action=\"store_true\", help=\"Rotation preset augmentation\")\n",
    "    parser.add_argument('--spawner', default=False, action=\"store_true\", help=\"SPAWNER preset augmentation\")\n",
    "    parser.add_argument('--dtwwarp', default=False, action=\"store_true\", help=\"DTW warp preset augmentation\")\n",
    "    parser.add_argument('--shapedtwwarp', default=False, action=\"store_true\", help=\"Shape DTW warp preset augmentation\")\n",
    "    parser.add_argument('--wdba', default=False, action=\"store_true\", help=\"Weighted DBA preset augmentation\")\n",
    "    parser.add_argument('--discdtw', default=False, action=\"store_true\", help=\"Discrimitive DTW warp preset augmentation\")\n",
    "    parser.add_argument('--discsdtw', default=False, action=\"store_true\", help=\"Discrimitive shapeDTW warp preset augmentation\")\n",
    "    parser.add_argument('--extra_tag', type=str, default=\"\", help=\"Anything extra\")\n",
    "    \n",
    "    args = parser.parse_args([\"--use_amp\"])\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "    args.use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "    print(torch.cuda.is_available())\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(' ', '')\n",
    "        device_ids = args.devices.split(',')\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    print('Args in experiment:')\n",
    "    print_args(args)\n",
    "\n",
    "    if args.task_name == 'long_term_forecast':\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "    elif args.task_name == 'short_term_forecast':\n",
    "        Exp = Exp_Short_Term_Forecast\n",
    "    elif args.task_name == 'imputation':\n",
    "        Exp = Exp_Imputation\n",
    "    elif args.task_name == 'anomaly_detection':\n",
    "        Exp = Exp_Anomaly_Detection\n",
    "    elif args.task_name == 'classification':\n",
    "        Exp = Exp_Classification\n",
    "    else:\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            exp = Exp(args)  # set experiments\n",
    "            setting = '{}_sl{}'.format(\n",
    "                args.model,\n",
    "                args.seq_len)\n",
    "\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "    else:\n",
    "        setting = '{}_sl{}'.format(\n",
    "            args.model,\n",
    "            args.seq_len)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04d2ea78-b70b-4b04-b07b-944962fbec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = pd.read_csv(\"./results/classification_TimesNet_sl180_ll128_pl64_dm512_nh5_el2_dl1_df512_expand2_dc4_fc1_ebtimeF_dtTrue_test_0/predictions_label.csv\")\n",
    "pred_label.columns = [\"id\", \"label\"]\n",
    "pred_label.to_csv(\"../out_put/timesnet.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6a2deee-dc25-4411-be02-4203ffedf628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    501\n",
       "2    366\n",
       "1    288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0725a73b-e18a-49d6-8fea-59bfc47d21a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1155, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "474152c6-918f-439c-8a64-a78e9170fa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1155 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label\n",
       "0        0      2\n",
       "1        1      2\n",
       "2        2      1\n",
       "3        3      0\n",
       "4        4      1\n",
       "...    ...    ...\n",
       "1150  1150      1\n",
       "1151  1151      0\n",
       "1152  1152      0\n",
       "1153  1153      0\n",
       "1154  1154      1\n",
       "\n",
       "[1155 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3f302-d6dd-411e-9c24-24c3547060f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae6946-8f6d-4c04-b03c-d8e64a1edbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b9592-fe29-442e-832b-f9fa2385a959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
